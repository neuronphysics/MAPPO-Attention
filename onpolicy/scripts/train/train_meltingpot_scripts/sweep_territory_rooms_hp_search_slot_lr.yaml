method: bayes
name: "meltingpot_territory_rooms_slot_qsa_lr_bayes"

metric:
  name: overall_average_episode_reward
  goal: maximize

# Parameters section - includes both swept and fixed values
parameters:
  # === SWEPT PARAMETERS ===
  lr:
    distribution: log_uniform_values
    min: 5e-5
    max: 1e-4

  critic_lr:
    distribution: log_uniform_values
    min: 5e-5
    max: 1e-4

  # === FIXED PARAMETERS ===
  entropy_coef:
    value: 0.008
  entropy_final_coef:
    value: 0.004
  clip_param:
    value: 0.1
  max_grad_norm:
    value: 0.2
  gain:
    value: 0.01
  warmup_updates:
    value: 100000
  cooldown_updates:
    value: 100000
  entropy_anneal_duration:
    value: 600000
  value_loss_coef:
    value: 0.75
  gae_lambda:
    value: 0.95
  gamma:
    value: 0.995
  slot_attn_loss_coef:
    value: 0.0001
  unfreeze_episode:
    value: 0
  use_orthogonal_loss:
    value: True
  orthogonal_loss_coef:
    value: 5.0

program: ../train/train_meltingpot.py

command:
  - python
  - ${program}
  - --use_valuenorm
  - "False"
  - --use_popart
  - "True"
  - --use_gae
  - "True"
  - --env_name
  - "Meltingpot"
  - --algorithm_name
  - "mappo"
  - --experiment_name
  - "territory_rooms_nu_6_RIM_LSTM_slot_QSA"
  - --substrate_name
  - "territory__rooms"
  - --num_agents
  - "9"
  - --seed
  - "123"
  - --n_rollout_threads
  - "25"
  - --use_wandb
  - "True"
  - --user_name
  - "zsheikhb"
  - --wandb_name
  - "zsheikhb"
  - --share_policy
  - "False"
  - --use_centralized_V
  - "False"
  - --load_model
  - "False"
  - --use_attention
  - "True"
  - --attention_module
  - "RIM"
  - --rnn_attention_module
  - "LSTM"
  - --rim_num_units
  - "6"
  - --rim_topk
  - "5"
  - --hidden_size
  - "270"
  - --num_env_steps
  - "2500000"
  - --log_interval
  - "1"
  - --episode_length
  - "1000"
  - --downsample
  - "True"
  - --img_scale_factor
  - "1"
  - --world_img_scale_factor
  - "8"
  - --pretrain_slot_att
  - "False"
  - --slot_train_ep
  - "200"
  - --slot_pretrain_batch_size
  - "1000"
  - --slot_att_work_path
  - "./onpolicy/scripts/results/slot_att/"
  - --slot_att_load_model
  - "True"
  - --use_slot_att
  - "True"
  - --use_pos_encoding
  - "True"
  - --use_input_att
  - "False"
  - --use_com_att
  - "True"
  - --use_x_reshape
  - "True"
  - --slot_att_crop_repeat
  - "9"
  - --slot_log_fre
  - "10"
  - --collect_data
  - "False"
  - --collect_agent
  - "False"
  - --collect_world
  - "False"
  - --collect_data_ep_num
  - "20"
  - --no_train
  - "False"
  - --crop_size
  - "88"
  - --weight_decay
  - "0.01"
  - --use_orthogonal
  - "True"
  - --huber_delta
  - "10"
  - --grad_clip
  - "0.2"
  - --use_eval
  - "True"
  - --eval_episodes
  - "5"
  - --eval_interval
  - "200"
  - --drop_out
  - "0.2"
  - --fine_tuning_type
  - "Partial"
  - --lr_main
  - "0.00005"
  - --use_slot_attn_transformer_decoder
  - "False"
  - --weight_clip_beta
  - "1.0"
  - --use_sweep_wandb_hyper_search
  - "True"
  - ${args}
